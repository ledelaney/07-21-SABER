
+ you don't necessarily have a pre-concieved notion about how the distribution SHOULD look
+ your argument should come from a literature review and expert judgements
+ you can compare the overall distribution of causes to distributions from quesitons, as well as compare the distributions between response one and two
+ then you can make a case for why this is a valuable teaching tool that can strengthen students understanding and conceptual thinking
+ summarize academic background of students (see table 1 in that paper)


Validity and
reliability evidence for the SECM was gathered in alignment with the Standards for Educational and Psychological Testing (AERA et al. 2014); reliability refers to the
degree to which instrument measures are replicable, stable, and free from error, and validity refers to the degree to
which evidence and theory support the interpretation

Diferent conceptual frameworks exist for validity. Tis
study adopted a construct validity framework, which
encompasses the gathering of evidence in alignment with
several separate but interrelated categories (Messick 1995;
Campbell and Nehm 2013; AERA et al. 2014), specifcally:
(i) evidence based on test content (i.e., content validity),
evidence based on internal structure (i.e., internal structure validity); (iii) evidence based on relationships to other
variables (i.e., convergent and/or discriminant validity);
(iv) evidence based on response processes (i.e., substantive validity); (v) validity generalization (i.e., generalization
validity); and (vi) evidence of consequences. Many studies
are typically needed to capture the full range of evidence
needed to establish construct validity. In this study, we
investigate content validity, internal structure validity, convergent validity, and substantive validity.

To generate evidence based on test content for the
SECM, we used a literature review and expert judgments to specify the content domain, conceptualize the
target construct, and operationalize it in the form of
closed-response items. Content validity addresses the relevance and representativeness of test content in light of
the intended construct (AERA et al. 2014). Evidence for
content validity can involve logical or empirical analyses of the extent to which the test content represents the
intended content domain (AERA et al. 2014).



(a) Content-Oriented Evidence *
(b) Evidence Regarding Cognitive Processes *
(c) Evidence Regarding Internal Structure
(d) Evidence Regarding Relationships With Conceptually Related Constructs
(e) Evidence Regarding Relationships With Criteria
(f) Evidence Based on Consequences of Tests

(a) Content-Oriented Evidence
Standard 1.11
When the rationale for test score interpretation
for a given use rests in part on the appropriateness
of test content, the procedures followed in specifying and generating test content should be described and justified with reference to the intended
population to be tested and the construct the
test is intended to measure or the domain it is
intended to represent. If the definition of the
content sampled incorporates criteria such as
importance, frequency, or criticality, these criteria
should also be clearly explained and justified.
Comment: For example, test developers might
provide a logical structure that maps the items on
the test to the content domain, illustrating the
relevance of each item and the adequacy with
which the set of items represents the content domain. Areas of the content domain that are not
included among the test items could be indicated
as well. The match of test content to the targeted
domain in terms of cognitive complexity and the
accessibility of the test content to all members of
the intended population are also important considerations.


(b) Evidence Regarding Cognitive
Processes
Standard 1.12
If the rationale for score interpretation for a given
use depends on premises about the psychological
processes or cognitive operations of test takers,
then theoretical or empirical evidence in support
of those premises should be provided.When statements about the processes employed by observers
or scorers are part of the argument for validity,
similar information should be provided.
Comment: If the test specification delineates the
processes to be assessed, then evidence is needed
that the test items do, in fact, tap the intended
processes.

(d) Evidence Regarding Relationships
With Conceptually Related Constructs
Standard 1.16
When validity evidence includes empirical analyses
of responses to test items together with data on
other variables, the rationale for selecting the additional variables should be provided. Where appropriate and feasible, evidence concerning the
constructs represented by other variables, as well
as their technical properties, should be presented
or cited. Attention should be drawn to any likely
sources of dependence (or lack of independence)
among variables other than dependencies among
the construct(s) they represent.
Comment: The patterns of association between
and among scores on the test under study and
other variables should be consistent with theoretical
expectations. The additional variables might be
demographic characteristics, indicators of treatment
conditions, or scores on other measures. They
might include intended measures of the same
construct or of different constructs.The reliability
of scores from such other measures and the validity
of intended interpretations of scores from these
measures are an important part of the validity evidence for the test under study. If such variables
include composite scores, the manner in which the composites were constructed should beexplained
(e.g., transformation or standardization of the
variables, and weighting of the variables). In
addition to considering the properties of each
variable in isolation, it is important to guard
against faulty interpretations arising from spurious
sources of dependency among measures, including
correlated errors or shared variance dueto common
methods of measurement or common elements.